---
# Start monitoring on ALL nodes
- name: Start pidstat monitoring on worker nodes
  hosts: workers
  become: yes  # Need sudo to monitor root processes
  vars:
    results_dir: "/home/dietpi/Documents/Repositories/RPi-Cluster-Spark/tpch/results/pidstat"
    timestamp: "{{ lookup('pipe', 'date +%Y%m%d_%H%M%S') }}"
  
  tasks:
    - name: Create results directory
      file:
        path: "{{ results_dir }}"
        state: directory
        mode: '0755'
        owner: dietpi
        group: dietpi
    
    - name: Check what Java/Spark processes exist before starting
      shell: ps aux | grep -E "[j]ava|[s]park|[e]xecutor" || echo "No Java/Spark processes found yet"
      register: initial_procs
      changed_when: false
    
    - name: Display initial processes
      debug:
        msg: 
          - "Node: {{ inventory_hostname }}"
          - "{{ initial_procs.stdout_lines }}"
    
    - name: Start pidstat monitoring - ALL processes
      shell: |
        LOG_FILE="{{ results_dir }}/pidstat_{{ inventory_hostname }}_{{ timestamp }}.log"
        
        # Run pidstat with sudo to monitor root processes
        # Filter to only show java processes to reduce noise
        nohup pidstat -h -u -r -d 1 -C java > "$LOG_FILE" 2>&1 &
        
        PID=$!
        echo $PID > /tmp/pidstat_{{ inventory_hostname }}.pid
        
        sleep 1
        if kill -0 $PID 2>/dev/null; then
          echo "SUCCESS: pidstat started on {{ inventory_hostname }} (PID: $PID)"
          echo "Logging to: $LOG_FILE"
          # Make log file readable by regular user
          chown dietpi:dietpi "$LOG_FILE"
        else
          echo "FAILED: pidstat did not start"
          exit 1
        fi
      args:
        executable: /bin/bash
      register: pidstat_start
    
    - name: Display pidstat startup status
      debug:
        msg: "{{ pidstat_start.stdout_lines }}"

# Wait for Spark to start and processes to spawn
- name: Wait for Spark cluster to initialize
  hosts: all
  tasks:
    - name: Pause to let Spark start
      pause:
        seconds: 10
        prompt: "Waiting for Spark executors to spawn..."
    
    - name: Check for Spark processes after delay
      shell: ps aux | grep -E "[j]ava|[s]park|[e]xecutor" || echo "Still no Spark processes"
      register: delayed_procs
      changed_when: false
    
    - name: Display processes after delay
      debug:
        msg: 
          - "Node: {{ inventory_hostname }}"
          - "{{ delayed_procs.stdout_lines }}"

# Run the benchmark on server only
- name: Run TPC-H benchmark on server
  hosts: server
  become: no
  vars:
    app_dir: "{{ ansible_env.HOME }}/Documents/Repositories/RPi-Cluster-Spark/tpch"
  
  tasks:
    - name: Create application directory
      file:
        path: "{{ app_dir }}"
        state: directory
        mode: '0755'
    
    - name: Copy Python scripts
      copy:
        src: "{{ item }}"
        dest: "{{ app_dir }}/{{ item }}"
        mode: '0644'
      loop:
        - run_tpch.py
        - config.py
        - load_data.py
        - queries.py
      when: item is file
    
    - name: Run TPC-H benchmark script
      shell: |
        source /etc/profile.d/spark.sh
        python3 run_tpch.py
      args:
        chdir: "{{ app_dir }}"
        executable: /bin/bash
      environment:
        SPARK_HOME: /opt/spark
        PYTHONPATH: "{{ app_dir }}"
      register: tpch_output
      
    - name: Display TPC-H output
      debug:
        msg: "{{ tpch_output.stdout_lines }}"
        
    - name: Display any errors
      debug:
        msg: "{{ tpch_output.stderr_lines }}"
      when: tpch_output.stderr_lines | length > 0

# Check what was captured after benchmark
# - name: Verify pidstat captured data
#   hosts: all
#   tasks:
#     - name: Check log file size and content
#       shell: |
#         LOG_FILE=$(ls -t {{ results_dir }}/pidstat_*.log 2>/dev/null | head -1)
#         if [ -f "$LOG_FILE" ]; then
#           echo "=== Log file: $LOG_FILE ==="
#           echo "Size: $(stat -c%s "$LOG_FILE") bytes"
#           echo "Line count: $(wc -l < "$LOG_FILE")"
#           echo ""
#           echo "=== First 20 lines ==="
#           head -20 "$LOG_FILE"
#           echo ""
#           echo "=== Last 20 lines ==="
#           tail -20 "$LOG_FILE"
#         else
#           echo "No log file found!"
#         fi
#       args:
#         executable: /bin/bash
#       register: log_content
#       ignore_errors: yes
    
    # - name: Display log content
    #   debug:
    #     msg: "{{ log_content.stdout_lines }}"
    
    # - name: Final process check
    #   shell: ps aux | grep -E "[j]ava|[s]park|[e]xecutor" || echo "No processes at end"
    #   register: final_procs
    #   changed_when: false
    
    # - name: Display final processes
    #   debug:
    #     msg: "{{ final_procs.stdout_lines }}"

# Stop monitoring
- name: Stop pidstat monitoring on all nodes
  hosts: all
  become: yes  # Need sudo to kill root-owned pidstat
  
  tasks:
    - name: Stop pidstat processes
      shell: |
        if [ -f /tmp/pidstat_{{ inventory_hostname }}.pid ]; then
          PID=$(cat /tmp/pidstat_{{ inventory_hostname }}.pid)
          if kill -0 $PID 2>/dev/null; then
            kill $PID 2>/dev/null
            echo "Stopped pidstat on {{ inventory_hostname }} (PID: $PID)"
          else
            echo "pidstat already stopped"
          fi
          rm /tmp/pidstat_{{ inventory_hostname }}.pid
        fi
      args:
        executable: /bin/bash
      register: pidstat_stop
      ignore_errors: yes

# # Collect logs
# - name: Collect pidstat logs
#   hosts: workers
#   become: no
#   vars:
#     results_dir: "~/Documents/Repositories/RPi-Cluster-Spark/tpch/results/pidstat"
#     local_results: "/home/xuestella03/Documents/Repositories/RPi-Cluster-Spark/tpch/results/pidstat"
  
#   tasks:
#     - name: Create local results directory
#       local_action:
#         module: file
#         path: "{{ local_results }}"
#         state: directory
#         mode: '0755'
#       run_once: true
    
#     - name: Find pidstat log files on remote nodes
#       find:
#         paths: "{{ results_dir }}"
#         patterns: "pidstat_{{ inventory_hostname }}_*.log"
#       register: log_files
    
#     - name: Fetch pidstat logs from all nodes
#       fetch:
#         src: "{{ item.path }}"
#         dest: "{{ local_results }}/"
#         flat: yes
#       loop: "{{ log_files.files }}"
#       when: log_files.files | length > 0
    
#     - name: Delete transferred log files from remote nodes
#       file:
#         path: "{{ item.path }}"
#         state: absent
#       loop: "{{ log_files.files }}"
#       when: log_files.files | length > 0


- name: Collect pidstat logs
  hosts: workers
  become: no
  vars:
    results_dir: "/home/dietpi/Documents/Repositories/RPi-Cluster-Spark/tpch/results/pidstat"
    local_results: "/home/xuestella03/Documents/Repositories/RPi-Cluster-Spark/tpch/results/pidstat"
  
  tasks:
    - name: Create local results directory
      local_action:
        module: file
        path: "{{ local_results }}"
        state: directory
        mode: '0755'
      run_once: true
    
    - name: Find pidstat log files on remote nodes
      find:
        paths: "{{ results_dir }}"
        patterns: "pidstat_{{ inventory_hostname }}_*.log"
      register: log_files
    
    - name: Display found files for debugging
      debug:
        msg: 
          - "Node: {{ inventory_hostname }}"
          - "Search path: {{ results_dir }}"
          - "Files found: {{ log_files.files | length }}"
          - "File paths: {{ log_files.files | map(attribute='path') | list }}"
    
    - name: Fetch pidstat logs from all nodes
      fetch:
        src: "{{ item.path }}"
        dest: "{{ local_results }}/"
        flat: yes
      loop: "{{ log_files.files }}"
      when: log_files.files | length > 0
    
    - name: Show warning if no files found
      debug:
        msg: "WARNING: No pidstat log files found on {{ inventory_hostname }} in {{ results_dir }}"
      when: log_files.files | length == 0

    - name: Delete all pidstat logs from remote nodes
      shell: rm -f {{ results_dir }}/pidstat_*.log
      args:
        executable: /bin/bash
      register: cleanup
      ignore_errors: yes
    
    - name: Confirm cleanup
      debug:
        msg: "Cleaned up pidstat logs on {{ inventory_hostname }}"