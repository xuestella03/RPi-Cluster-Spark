# Start the Spark Master and Workers.
# Right now a lot of this stuff is hardcoded (JAVA_HOME especially) 
# so I need to fix that.

# Also, killing the process and restarting the spark master fails
# every other time (the starting after the stop fails)
# need to fix

- name: Start Spark master
  hosts: server
  become: yes
  tasks:
    # - name: Get JAVA_HOME and save 
    #   # ansible.builtin.shell: |
    #   #   source /etc/profile
    #   #   echo $JAVA_HOME 
    #   # args:
    #   #   executable: /bin/bash
    #   # register: master_java_home_output
    #   include_vars:
    #     file: ansible/inventory/group_vars/server.yml
    #     name: server_vars
    # - name: Display java home output 
    #   ansible.builtin.debug:
    #     # var: master_java_home_output
    #     msg: "{{ server_vars.java_home }}"

    - name: Get SPARK_HOME and save 
      ansible.builtin.shell: |
        source /etc/profile.d/spark.sh
        echo $SPARK_HOME 
      args:
        executable: /bin/bash
      register: master_spark_home_output
    - name: Display spark home output 
      ansible.builtin.debug:
        var: master_spark_home_output.stdout 

    - name: Update spark-env.sh with server JAVA_HOME 
      copy:
        dest: "{{ master_spark_home_output.stdout }}/conf/spark-env.sh"
        content: |
          #!/usr/bin/env bash
          export JAVA_HOME=/usr/lib/jvm/java-1.21.0-openjdk-amd64
          export SPARK_MASTER_HOST={{ ansible_default_ipv4.address }}
          export SPARK_MASTER_PORT=7077
          export SPARK_MASTER_WEBUI_PORT=8080
        mode: '0755'
        
    - name: Kill any existing Spark processes
      shell: pkill -f org.apache.spark.deploy
      ignore_errors: yes
      
    - name: Start Spark master
      shell: |
        export SPARK_HOME={{ master_spark_home_output.stdout }}
        export JAVA_HOME=/usr/lib/jvm/java-1.21.0-openjdk-amd64
        {{ master_spark_home_output.stdout }}/sbin/start-master.sh
        sleep 5
      register: master_start
      
    - name: Check if master is running
      shell: ps aux | grep org.apache.spark.deploy.master.Master | grep -v grep
      register: master_check
      
    - name: Display master process
      debug:
        msg: "Master is running"
      
    - name: Check if port 7077 is listening
      wait_for:
        host: "{{ ansible_default_ipv4.address }}"
        port: 7077
        timeout: 10
        
    - name: Success message
      debug:
        msg: 
          - "Spark Master started successfully!"
          - "Master URL: spark://{{ ansible_default_ipv4.address }}:7077"
          - "Web UI: http://{{ ansible_default_ipv4.address }}:8080"

- name: Start Spark workers
  hosts: workers
  become: yes
  vars:
    worker_spark_home: /opt/spark
    # worker_java_home: /usr/lib/jvm/java-21-openjdk-armhf
    worker_java_home: /opt/liberica-jdk21
  tasks:
    # - name: Get JAVA_HOME and save 
    #   ansible.builtin.shell:  |
    #     source /etc/profile.d/java.sh
    #     echo $JAVA_HOME 
    #   args:
    #     executable: /bin/bash
    #   register: worker_java_home_output
    # - name: Display java home output 
    #   ansible.builtin.debug:
    #     var: worker_java_home_output.stdout

    # - name: Get SPARK_HOME and save 
    #   ansible.builtin.shell:  |
    #     source /etc/profile.d/spark.sh
    #     echo $SPARK_HOME 
    #   args:
    #     executable: /bin/bash
    #   register: worker_spark_home_output
    # - name: Display spark home output 
    #   ansible.builtin.debug:
    #     var: worker_spark_home_output.stdout 

    - name: Update spark-env.sh for workers
      copy:
        dest: "{{ worker_spark_home }}/conf/spark-env.sh"
        content: |
          #!/usr/bin/env bash
          export JAVA_HOME={{ worker_java_home }}
          export SPARK_WORKER_CORES=4
          export SPARK_WORKER_MEMORY=4g
        mode: '0755'
        
    - name: Get master IP
      set_fact:
        master_ip: "{{ hostvars[groups['server'][0]]['ansible_default_ipv4']['address'] }}"
        
    - name: Kill any existing Spark worker processes
      shell: pkill -f org.apache.spark.deploy.worker
      ignore_errors: yes
      
    - name: Start Spark worker
      shell: |
        export SPARK_HOME={{ worker_spark_home }}
        export JAVA_HOME={{ worker_java_home }}
        {{ worker_spark_home }}/sbin/start-worker.sh spark://{{ master_ip }}:7077
        sleep 3
      register: worker_start
      
    - name: Check if worker is running
      shell: ps aux | grep org.apache.spark.deploy.worker.Worker | grep -v grep
      register: worker_check
      
    - name: Display worker process
      debug:
        msg: "Worker is running"
      when: worker_check.rc == 0
        
    - name: Show worker log if failed
      shell: tail -50 {{ worker_spark_home }}/logs/*Worker*.out
      register: worker_log
      when: worker_check.rc != 0
      ignore_errors: yes
      
    - name: Display worker log on failure
      debug:
        msg: "{{ worker_log.stdout_lines }}"
      when: worker_check.rc != 0
        
    - name: Success message
      debug:
        msg: "Worker connected to spark://{{ master_ip }}:7077"
      when: worker_check.rc == 0
- name: Wait for cluster to be fully ready
  hosts: server
  tasks:
    - name: Wait for master web UI
      wait_for:
        port: 8080
        delay: 5
        timeout: 30
        
    - name: Pause for workers to register
      pause:
        seconds: 15
        prompt: "Waiting for workers to register..."
