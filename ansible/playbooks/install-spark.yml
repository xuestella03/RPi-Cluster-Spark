# PySpark is supported by cluster mode in the sense that it can 
# submit scripts to the cluster manager.

# However, we do need Spark itself to build the cluster. 
# So download from Spark website.
# Then Spark will persist in each node (so our other option here
# is to just download into the base image). 


- name: Download Spark on all nodes
  hosts: all
  become: yes
  tasks:
    # - name: Install basic dependencies
    #   apt:
    #     name:
    #       - tar
    #       - gzip
    #     update_cache: yes
    - name: Remove any existing tarball
      file:
        path: /tmp/spark.tar.gz
        state: absent
        
    - name: Download Spark tarball on each node
      get_url:
        # url: https://archive.apache.org/dist/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz
        url: https://dlcdn.apache.org/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz
        dest: /tmp/spark.tar.gz
        mode: '0644'
        timeout: 300
      register: download_result
      
    - name: Create Spark installation directory
      file:
        path: /opt/spark
        state: directory
        mode: '0755'
        
    - name: Extract Spark
      unarchive:
        src: /tmp/spark.tar.gz
        dest: /opt/spark
        remote_src: yes
        extra_opts: [--strip-components=1]
        creates: /opt/spark/bin/spark-submit
        
    - name: Set SPARK_HOME environment variable
      lineinfile:
        path: /etc/profile.d/spark.sh
        line: 'export SPARK_HOME=/opt/spark'
        create: yes
        mode: '0644'
        
    - name: Add Spark to PATH
      lineinfile:
        path: /etc/profile.d/spark.sh
        line: 'export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin'
        
    - name: Set SPARK_HOME in current session
      shell: echo 'export SPARK_HOME=/opt/spark' >> ~/.bashrc
      become: no
      
    - name: Clean up tarball
      file:
        path: /tmp/spark.tar.gz
        state: absent